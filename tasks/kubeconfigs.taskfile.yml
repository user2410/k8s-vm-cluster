version: '3'

tasks:
  dist:
    desc: "Distribute encryption key to VMs"
    status:
      - |
        # Check if ALL of the kubeconfig files exist
        for prefix in admin worker1 worker2 kube-proxy kube-scheduler kube-controller-manager kube-api-server; do
          if [ ! -f "./kubeconfigs/${prefix}.kubeconfig" ]; then
            exit 0  # File not found → skip it
          fi
        done
        exit 1  # All files found → run it
    cmds:
      - for: ['worker1', 'worker2']
        cmd: |
          echo "Copy kube-proxy kubeconfig files to each worker instance"
          echo "Extracting variable..."
          WORKER_IP=$(grep '{{.ITEM}}' ./scripts/local/hosts | awk '{print $1}')
          echo "Extracted WORKER_IP: $WORKER_IP"

          scp -i .vagrant/machines/{{.ITEM}}/virtualbox/private_key ./kubeconfigs/kube-proxy.kubeconfig vagrant@$WORKER_IP:~/
      - |
          echo "Copy admin.kubeconfig, kube-controller-manager and kube-scheduler kubeconfig files to each controller instance"
          echo "Extracting variable..."
          MASTER_IP=$(grep '{{.ITEM}}' ./scripts/local/hosts | awk '{print $1}')
          echo "Extracted MASTER_IP: $MASTER_IP"

          scp -i .vagrant/machines/master/virtualbox/private_key \ 
            ./kubeconfigs/admin.kubeconfig \
            ./kubeconfigs/kube-controller-manager.kubeconfig \
            ./kubeconfigs/kube-scheduler.kubeconfig \
            vagrant@$MASTER_IP:~/

  gen:all:
    desc: Generate kubeconfig files for Kubernetes components
    vars:
      CLUSTER_NAME: '{{.CLUSTER_NAME | default "k8s-cluster"}}'
    status:
      - |
        if command -v kubectl >/dev/null 2>&1; then
          exit 1
        fi
        exit 0
    cmds:
      - task: gen:single
        vars: { CLUSTER_NAME: "{{.CLUSTER_NAME}}", NAME: "worker1", USER: "system:node:worker1", CERT: "worker1" }
      - task: gen:single
        vars: { CLUSTER_NAME: "{{.CLUSTER_NAME}}", NAME: "worker2", USER: "system:node:worker2", CERT: "worker2" }
      - task: gen:single
        vars: { CLUSTER_NAME: "{{.CLUSTER_NAME}}", NAME: "kube-proxy", USER: "system:kube-proxy", CERT: "kube-proxy" }
      - task: gen:single
        vars: { CLUSTER_NAME: "{{.CLUSTER_NAME}}", NAME: "kube-controller-manager", USER: "system:kube-controller-manager", CERT: "kube-controller-manager" }
      - task: gen:single
        vars: { CLUSTER_NAME: "{{.CLUSTER_NAME}}", NAME: "kube-scheduler", USER: "system:kube-scheduler", CERT: "kube-scheduler" }
      - task: gen:single
        vars: { CLUSTER_NAME: "{{.CLUSTER_NAME}}", NAME: "admin", USER: "admin", CERT: "admin" }

  gen:single:
    internal: true
    vars:
      CLUSTER_NAME: '{{.CLUSTER_NAME}}'
      NAME: '{{.NAME}}'
      USER: '{{.USER}}'
      CERT: '{{.CERT}}'
      CERT_PATH: './certs'
      KUBECONFIG_PATH: './kubeconfigs/{{.NAME}}.kubeconfig'
    status:
      - test -f {{.KUBECONFIG_PATH}}
    cmds:
      - |
        echo "Generating kubeconfig for {{.NAME}}"
        kubectl config set-cluster {{.CLUSTER_NAME}} \
          --certificate-authority={{.CERT_PATH}}/ca.crt \
          --embed-certs=true \
          --server=https://master.kubernetes.local:6443 \
          --kubeconfig={{.KUBECONFIG_PATH}}
        kubectl config set-credentials {{.USER}} \
          --client-certificate={{.CERT_PATH}}/{{.CERT}}.crt \
          --client-key={{.CERT_PATH}}/{{.CERT}}.key \
          --embed-certs=true \
          --kubeconfig={{.KUBECONFIG_PATH}}
        kubectl config set-context default \
          --cluster={{.CLUSTER_NAME}} \
          --user={{.USER}} \
          --kubeconfig={{.KUBECONFIG_PATH}}
        kubectl config use-context default \
          --kubeconfig={{.KUBECONFIG_PATH}}

  clean:
    desc: "Clean up kubeconfig files"
    cmds:
      - |
        echo "Cleaning up kubeconfig files..."
        rm -f ./kubeconfigs/*.kubeconfig
        echo "Kubeconfig files cleaned up."
